import os
import time
import csv
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

BASE_URL = "https://divar.ir"
CATEGORY_URL = f"{BASE_URL}/s/tehran/services"
AD_COUNT = 20

class DivarScraper:
    def __init__(self):
        self.driver = None
        self.wait = None
        self.data = []

    def initialize_driver(self):
        options = Options()
        options.add_argument("--disable-blink-features=AutomationControlled")
        self.driver = webdriver.Edge(options=options)
        self.driver.maximize_window()
        self.wait = WebDriverWait(self.driver, 30)

    def wait_for_login(self):
        print("â³ Ù„Ø·ÙØ§Ù‹ Ø¯Ø± Ù…Ø±ÙˆØ±Ú¯Ø± ÙˆØ§Ø±Ø¯ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø´ÙˆÛŒØ¯...")
        self.driver.get(BASE_URL)
        input("âœ… Ù¾Ø³ Ø§Ø² ÙˆØ±ÙˆØ¯ØŒ Enter Ø±Ø§ ÙØ´Ø§Ø± Ø¯Ù‡ÛŒØ¯ ")

    def go_to_services(self):
        self.driver.get(CATEGORY_URL)
        print("âœ… ÙˆØ§Ø±Ø¯ Ø¨Ø®Ø´ Ø®Ø¯Ù…Ø§Øª Ø´Ø¯")

    def load_all_ads(self):
        """Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ ØªÙ…Ø§Ù… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§"""
        last_height = self.driver.execute_script("return document.body.scrollHeight")
        while True:
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = self.driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

    def collect_ad_links(self):
        try:
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ù„Ú©ØªÙˆØ± ØµØ­ÛŒØ­ Ø¨Ø±Ø§ÛŒ Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§
            self.wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, 'div.post-card-item')
            ))
            
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ ØªÙ…Ø§Ù… Ø¢Ú¯Ù‡ÛŒâ€ŒÙ‡Ø§
            self.load_all_ads()
            
            ads = self.driver.find_elements(By.CSS_SELECTOR, 'a.kt-post-card')[:AD_COUNT]
            return [ad.get_attribute('href') for ad in ads if ad.get_attribute('href')]
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {e}")
            return []

    def extract_phone_number(self):
        try:
            contact_btn = self.wait.until(EC.element_to_be_clickable(
                (By.CSS_SELECTOR, 'button.post-actions__get-contact')
            ))
            contact_btn.click()
            
            self.wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, 'div.kt-contact-row')
            ))
            
            phone_element = self.driver.find_element(
                By.CSS_SELECTOR, 
                'a[href^="tel:"]'
            )
            return phone_element.get_attribute('href').replace('tel:', '')
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ù…Ø§Ø±Ù‡: {e}")
            return "ÛŒØ§ÙØª Ù†Ø´Ø¯"

    def process_ad_page(self, url):
        self.driver.get(url)
        try:
            title = self.wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, 'h1.kt-page-title__title')
            )).text.strip()
        except:
            title = "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
            
        phone = self.extract_phone_number()
        
        self.data.append({
            "title": title,
            "phone": phone,
            "url": url
        })
        print(f"âœ… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {title}")

    def save_results(self):
        os.makedirs("result", exist_ok=True)
        with open("result/ads.csv", "w", encoding="utf-8", newline='') as f:
            writer = csv.DictWriter(f, fieldnames=["title", "phone", "url"])
            writer.writeheader()
            writer.writerows(self.data)
        print("âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")

    def run(self):
        try:
            self.initialize_driver()
            self.wait_for_login()
            self.go_to_services()
            
            links = self.collect_ad_links()
            print(f"ğŸ” ØªØ¹Ø¯Ø§Ø¯ {len(links)} Ø¢Ú¯Ù‡ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
            
            for i, link in enumerate(links, 1):
                print(f"Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¢Ú¯Ù‡ÛŒ {i}/{len(links)}...")
                self.process_ad_page(link)
                time.sleep(1)
            
            self.save_results()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§: {e}")
        finally:
            if self.driver:
                self.driver.quit()

if __name__ == "__main__":
    scraper = DivarScraper()
    scraper.run()